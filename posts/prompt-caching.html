<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>提示词缓存技术详解 - Leochame</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.0/font/bootstrap-icons.css">
    <link rel="stylesheet" href="../css/main.css">
    <link rel="icon" href="../images/favicon.ico">
    <meta name="description" content="详细阐述提示词缓存（Prompt Caching）技术的定义、机制、运作流程、收益与应用">
    <meta name="keywords" content="提示词缓存, Prompt Caching, LLM, 大语言模型, AI性能优化">
    <style>
        .article-content {
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem 1rem;
        }
        
        .article-content h1 {
            text-align: center;
            line-height: 1.75;
            font-size: 2rem;
            display: table;
            padding: 0.5em 1em;
            border-bottom: 2px solid var(--primary-color);
            margin: 2em auto 1em;
            font-weight: bold;
            text-shadow: 1px 1px 3px rgba(0,0,0,0.05);
        }
        
        .article-content h2 {
            text-align: center;
            line-height: 1.75;
            font-size: 1.5rem;
            display: table;
            padding: 0.3em 1.2em;
            margin: 4em auto 2em;
            color: #fff;
            background: var(--primary-color);
            font-weight: bold;
            border-radius: 8px 24px 8px 24px;
            box-shadow: 0 2px 6px rgba(0,0,0,0.06);
        }
        
        .article-content p {
            text-align: left;
            line-height: 1.75;
            margin: 1.5em 0;
            letter-spacing: 0.05em;
        }
        
        .article-content blockquote {
            font-style: italic;
            padding: 1em 1em 1em 2em;
            border-left: 4px solid var(--primary-color);
            border-radius: 6px;
            color: rgba(0,0,0,0.6);
            background: var(--hover-color);
            margin-bottom: 1em;
            border-bottom: 0.2px solid rgba(0, 0, 0, 0.04);
            border-top: 0.2px solid rgba(0, 0, 0, 0.04);
            border-right: 0.2px solid rgba(0, 0, 0, 0.04);
        }
        
        .article-content ul {
            padding-left: 1.5em;
        }
        
        .article-content img {
            display: block;
            max-width: 100%;
            margin: 1.5em auto;
            border-radius: 8px;
            border: 1px solid rgba(0, 0, 0, 0.04);
        }
        
        .article-content table {
            width: 100%;
            margin: 1.5em 0;
            border-collapse: collapse;
        }
        
        .article-content th, .article-content td {
            border: 1px solid #dfdfdf;
            padding: 0.5em;
        }
        
        .article-content th {
            background: rgba(0, 0, 0, 0.05);
            font-weight: bold;
        }
        
        .article-content code {
            background: rgba(27, 31, 35, 0.05);
            padding: 3px 5px;
            border-radius: 4px;
            color: var(--primary-color);
        }
        
        .article-content strong {
            color: var(--primary-color);
            font-weight: bold;
        }
        
        .article-header {
            text-align: center;
            margin-bottom: 2rem;
        }
        
        .article-header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        .article-meta {
            color: var(--secondary-color);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .article-meta span {
            margin: 0 0.5rem;
        }
        
        .article-tags {
            margin: 1rem 0;
        }
        
        .article-tag {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            background-color: var(--hover-color);
            color: var(--primary-color);
            border-radius: 30px;
            font-size: 0.8rem;
            margin: 0 0.25rem;
        }
    </style>
</head>
<body>
<header class="header">
    <div class="container">
        <nav class="navbar navbar-expand-lg navbar-light">
            <div class="container-fluid">
                <a class="navbar-brand" href="../index.html">Leochame</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarNav">
                    <ul class="navbar-nav ms-auto">
                        <li class="nav-item">
                            <a class="nav-link" href="../index.html">首页</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../index.html#featured-articles">推荐文章</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../tech.html">技术专栏</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../research.html">研究札记</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../about.html">关于我</a>
                        </li>
                        <li class="nav-item d-flex align-items-center">
                            <i class="bi bi-sun-fill theme-icon"></i>
                            <label class="theme-switch">
                                <input type="checkbox" id="theme-switch">
                                <span class="slider"></span>
                            </label>
                            <i class="bi bi-moon-fill theme-icon"></i>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>
    </div>
</header>

<section class="hero-section fade-in-up">
    <div class="container">
        <div class="row">
            <div class="col-12 text-center">
                <div class="hero-content">
                    <h1>提示词缓存技术详解</h1>
                    <p class="lead">深入探究大型语言模型的关键性能优化技术</p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section-compact fade-in-up">
    <div class="container">
        <div class="article-header">
            <div class="article-meta">
                <span><i class="bi bi-calendar3"></i> 2025-07-28</span>
                <span><i class="bi bi-person"></i> Leochame</span>
                <span><i class="bi bi-eye"></i> 技术研究</span>
            </div>
            <div class="article-tags">
                <span class="article-tag">LLM</span>
                <span class="article-tag">性能优化</span>
                <span class="article-tag">提示词工程</span>
            </div>
        </div>
        
        <div class="article-content">
            <h1 id="0">导读</h1>
            <p>本文旨在详细阐述"提示词缓存"（Prompt Caching）技术。为确保内容的清晰与循序渐进，文章将首先介绍其基本定义与核心机制，随后在文末部分深入探讨其在大型语言模型（LLM）内部的技术实现原理，供有进阶需求的读者参考。</p>
            
            <blockquote>
                <p>注：笔者在大型语言模型领域的知识尚在学习和探索中，文中若有任何谬误之处，恳请不吝指正。</p>
            </blockquote>
            
            <hr>
            
            <p>笔者对提示词缓存的最初了解，源于 Sharon Li 在亚马逊云科技中国峰会上的演讲《提示缓存 (Prompt Caching) 和跨区域推理 (Cross-Region Inference)》。此后，通过研读 Manus 联合创始人兼首席科学家季逸超（Yichao 'Peak' Ji）的文章《Context Engineering for AI Agents: Lessons from Building Manus》及《LLMs-from-scratch》一书，笔者对该技术获得了更深层次的认知，并在此基础上撰写本文。</p>
            
            <h1 id="1">提示词缓存的定义与机制</h1>
            <p>提示词缓存是内嵌于大型语言模型的一种关键性能优化技术。其核心思想在于，保存并复用模型对文本前缀（prefix）进行处理时产生的中间计算结果，从而显著提升处理后续相关任务时的效率。</p>
            
            <p>当模型处理请求时，它缓存的并非原始文本，而是文本经过计算后生成的、代表其深层上下文含义的一组关键数据——"键值对"（Key-Value Pairs）。这组数据通常被称为"上下文状态"或"KV Cache"。</p>
            
            <p>缓存的激活与管理，既有由模型根据预设规则（如提示长度）自动触发，也存在由用户通过 API 参数进行手动指定。无论采用何种方式，其核心运作原理是相同的：当模型识别到一个新请求的前缀与已缓存内容完全相同时，便能直接加载对应的"KV Cache"，从而跳过对该前缀部分的重复计算，直接从新的、未缓存的文本部分开始处理，显著提升效率。</p>
            
            <p>"重复计算"之所以成为性能瓶颈，源于当前主流大模型（多基于 Transformer 架构）的"注意力机制"。在生成每一个新词时，模型都需要回顾并处理此前已生成的所有文本。例如，在无缓存的情况下，生成第100个词，模型需要重新处理前99个词的内部状态，这无疑造成了巨大的计算资源浪费。</p>
            
            <p>提示词缓存正是为解决此问题而设计。因此，其生效条件也极为严格：请求的前缀必须与缓存中的前缀实现百分之百的精确匹配。任何细微的差异，都会导致上下文信息改变，从而使缓存失效。</p>
            
            <figure>
                <img src="../images/posts/prompt-caching-1.svg" alt="提示词缓存示意图" class="img-fluid">
                <figcaption class="text-center text-muted">提示词缓存示意图</figcaption>
            </figure>
            
            <h1 id="2">提示词缓存的运作流程</h1>
            <p>提示词缓存的运作流程有从用户手动指定的，也有模型自动触发的场景。</p>
            
            <p>在手动模式下，用户可以明确指定需要缓存的提示部分（即提示词前缀），当将这个前缀发送给模型时，模型会处理输入并将处理后的上下文保存在缓存中。而在自动模式下，模型则会根据内部规则（例如，当提示词超过一定长度时）自行完成这一过程。根据 Open AI 的数据，提示词缓存在不活跃5-10分钟后失效，但在非高峰时段最长可达1小时。</p>
            
            <figure>
                <img src="../images/posts/prompt-caching-2.svg" alt="提示词缓存运作流程图" class="img-fluid">
                <figcaption class="text-center text-muted">提示词缓存运作流程</figcaption>
            </figure>
            
            <p>对于包含相同提示前缀的后续请求，模型会直接加载先前缓存的 <strong>上下文状态（KV Cache）</strong>，从而跳过对该前缀的重复计算。每次成功的缓存读取都会刷新其生命周期（Live Time）。</p>
            
            <figure>
                <img src="../images/posts/prompt-caching-3.svg" alt="缓存读取流程图" class="img-fluid">
                <figcaption class="text-center text-muted">缓存读取流程</figcaption>
            </figure>
            
            <h1 id="3">提示词缓存带来的收益</h1>
            <blockquote>
                <p>With prompt caching, customers can provide Claude with more background knowledge and example outputs—all while reducing costs by up to 90% and latency by up to 85% for long prompts.</p>
                <p>-Anthropic  《Prompt caching with Claude》</p>
            </blockquote>
            
            <p>Anthropic 团队在《Prompt caching with Claude》一文中给出的数据，可以看到提示词缓存带来了两大好处，更快、更便宜：</p>
            
            <ul>
                <li><strong>显著降低延迟</strong>：通过避免对相同内容的冗余处理，模型的响应时间得以大幅提升。报告显示，响应速度最高可提升 85%。
                    <blockquote>
                        <p>这对于需要即时反馈的交互式应用（如聊天机器人）和延迟会累积的多步代理工作流（agentic workflows）而言至关重要。</p>
                    </blockquote>
                </li>
                <li><strong>大幅削减成本</strong>：这是提示词缓存最引人注目的优势。复用与词元（Token）关联的计算状态（KV Cache）远比重新处理它们要便宜得多。
                    <blockquote>
                        <p>这里的"延迟"指的是<strong>首字符响应时间</strong>（<strong>TTFT</strong>，<strong>Time to First Token</strong>），它计算的是从用户发送请求（Prompt）到模型生成并返回第一个输出词元（Token）所花费的时间。TTFT 越短，代表模型的反应越快，用户感受到的延迟就越低。</p>
                    </blockquote>
                </li>
            </ul>
            
            <p>下表汇总了两大供应商缓存功能的关键特性：</p>
            
            <table class="table table-bordered">
                <thead>
                    <tr>
                        <th>特性</th>
                        <th>OpenAI (gpt-4o 系列)<br><span class="text-muted">Prompt Caching in the API</span></th>
                        <th>Anthropic (Claude 系列)<br><span class="text-muted">Prompt caching - Anthropic</span></th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>激活方式</strong></td>
                        <td>自动（提示词 &gt; 1024 tokens）</td>
                        <td>手动（通过 API 中的 <code>cache_control</code>）</td>
                    </tr>
                    <tr>
                        <td><strong>成本模型</strong></td>
                        <td>写入无额外成本，读取享折扣</td>
                        <td>写入溢价，读取享更高折扣</td>
                    </tr>
                    <tr>
                        <td><strong>缓存写入成本</strong></td>
                        <td>无额外费用</td>
                        <td>+25% 溢价</td>
                    </tr>
                    <tr>
                        <td><strong>缓存读取折扣</strong></td>
                        <td>最高 50%</td>
                        <td>90%</td>
                    </tr>
                    <tr>
                        <td><strong>生命周期 (TTL)</strong></td>
                        <td>5-10 分钟（非高峰可达 1 小时）</td>
                        <td>5 分钟（使用时刷新），可配置 1 小时</td>
                    </tr>
                    <tr>
                        <td><strong>手动控制</strong></td>
                        <td>有限（通过 <code>user</code> 参数影响路由）</td>
                        <td>灵活（最多 4 个缓存断点）</td>
                    </tr>
                    <tr>
                        <td><strong>监控字段</strong></td>
                        <td><code>prompt_tokens_details.cached_tokens</code></td>
                        <td><code>cache_creation_input_tokens</code>,<br><code>cache_read_input_tokens</code></td>
                    </tr>
                </tbody>
            </table>
            
            <h1 id="4">提示词缓存的应用</h1>
            <p>根据提示词缓存的工作原理，其应用的<strong>核心思想</strong>在于：<strong>将稳定、可复用的内容置于提示词前部进行缓存，而将动态变化的内容（如用户单次提问）置于末尾</strong>。</p>
            
            <p>另外，由于单个标记的差异也会使该标记之后的缓存失效，这使得我们使用的时候尽量偏向于 append 操作，同时要确保我们的序列化是确定性的（有些编程语言和库在序列化JSON对象时不保证键顺序的稳定性，这可破坏缓存）。</p>
            
            <p>此外，为确保缓存有效，还需要注意以下两点：</p>
            
            <ul>
                <li><strong>追加式修改</strong>：由于任何微小变动都会使该位置之后的缓存失效，因此对提示词的修改应尽可能采用向末尾追加（append-only）的方式。</li>
                <li><strong>确定性序列化</strong>：在将结构化数据（如 JSON）转换为字符串时，必须保证其<strong>序列化过程是确定性的</strong>。例如，应始终保持 JSON 对象中键的顺序稳定，否则即使内容相同，序列化的结果也可能不同，从而导致缓存失效。</li>
            </ul>
            
            <h2 id="5">推荐缓存内容</h2>
            <p>在 Claude 的官方文档中，Anthropic 推荐将以下几类内容纳入缓存：</p>
            
            <blockquote>
                <ul>
                    <li><strong>Tools</strong>: Tool definitions in the <code>tools</code> parameter.</li>
                    <li><strong>System prompts</strong>: Instructions in the <code>system</code> parameter.</li>
                    <li><strong>Few-shot examples</strong>: Example conversations in the <code>messages</code> parameter.</li>
                    <li><strong>Context window content</strong>: Documents, knowledge bases, or other reference material.</li>
                </ul>
            </blockquote>
            
            <p>这些内容通常在多次请求中保持不变，特别适合缓存处理。例如，在构建一个专业领域的助手时，可以将领域知识、工具定义、系统提示等放在前缀部分进行缓存，而将用户的实时问题放在末尾。</p>
            
            <p>以下是一个简化的示例，展示了如何在实际应用中组织提示词以便于缓存：</p>
            
            <pre><code>// 缓存部分（前缀）
{
  "system": "你是一位专业的医疗顾问，擅长解答健康相关问题...",
  "tools": [...],  // 工具定义
  "context": "大量医学知识库内容...",
  "few_shot_examples": [...]  // 示例对话
}

// 非缓存部分（末尾）
{
  "user_query": "我最近感到头痛，可能是什么原因？"
}</code></pre>
            
            <p>在这个结构中，只有 <code>user_query</code> 部分会随着用户的不同问题而变化，而前面的系统提示、工具定义、知识库内容等都可以被缓存，从而在处理大量用户请求时显著提高效率。</p>
            
            <h1 id="6">总结与展望</h1>
            <p>提示词缓存作为大型语言模型的重要性能优化技术，通过复用已处理内容的计算状态，有效解决了 Transformer 架构中注意力机制带来的计算冗余问题。它不仅显著降低了模型的响应延迟，还大幅削减了 API 调用成本，为构建高效、经济的 AI 应用提供了强有力的支持。</p>
            
            <p>随着大模型应用的普及与深入，提示词缓存技术也在不断演进。未来，我们可能会看到更加智能的缓存策略、更灵活的缓存控制机制，以及与其他优化技术（如量化、模型蒸馏等）的协同应用，进一步提升大模型的性能与可用性。</p>
            
            <p>对于开发者而言，深入理解并合理利用提示词缓存，将成为构建高效 AI 应用的重要技能。通过精心设计提示词结构，将稳定内容前置，动态内容后置，可以最大化缓存带来的性能收益，为用户提供更加流畅、响应迅速的 AI 体验。</p>
        </div>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6">
                <div class="d-flex align-items-center mb-3">
                    <h5 class="me-3 mb-0">Leochame</h5>
                    <div class="footer-social">
                        <a href="https://github.com/leochame" target="_blank"><i class="bi bi-github"></i></a>
                        <a href="https://juejin.cn/user/3672826532608270" target="_blank"><i class="bi bi-link-45deg"></i></a>
                        <a href="mailto:liulch.cn@gmail.com"><i class="bi bi-envelope"></i></a>
                    </div>
                </div>
                <p class="small">后端开发工程师，专注于Java生态系统、系统架构设计与性能优化</p>
            </div>
            <div class="col-md-6">
                <div class="row">
                    <div class="col-6">
                        <h6>导航</h6>
                        <ul class="footer-links small">
                            <li><a href="../tech.html">技术专栏</a></li>
                            <li><a href="../research.html">研究札记</a></li>
                            <li><a href="../about.html">关于我</a></li>
                        </ul>
                    </div>
                    <div class="col-6">
                        <h6>热门分类</h6>
                        <ul class="footer-links small">
                            <li><a href="../tech.html?category=Java">Java开发</a></li>
                            <li><a href="../tech.html?category=数据库">数据库</a></li>
                            <li><a href="../research.html?category=操作系统">操作系统</a></li>
                            <li><a href="../research.html?category=计算机基础">计算机基础</a></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="footer-bottom text-center mt-3">
            <p class="small mb-0">&copy; 2023-2025 Leochame 的技术博客。使用 <a href="https://pages.github.com/">GitHub Pages</a> 构建。</p>
        </div>
    </div>
</footer>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js" defer></script>
<script src="../js/theme-switcher.js" defer></script>
<script src="../js/main.js" defer></script>
</body>
</html> 